{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformação de dados - Large Office"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidando dados de consumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processamento concluído! Arquivo salvo em: C:\\Users\\tbekho01.ATKEARNEY_AD\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\24. Handover\\250826_Otimização de Abastecedores_vEnviado\\Dados Raw\\Consumo\\Consumo_Consolidado.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Caminhos das pastas\n",
    "# -----------------------------\n",
    "pasta_rateio = r\"C:\\Users\\tbekho01.ATKEARNEY_AD\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\24. Handover\\250826_Otimização de Abastecedores_vEnviado\\Dados Raw\\Consumo\\Rateio\"\n",
    "pasta_telemetria = r\"C:\\Users\\tbekho01.ATKEARNEY_AD\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\24. Handover\\250826_Otimização de Abastecedores_vEnviado\\Dados Raw\\Consumo\\Telemetria\"\n",
    "arquivo_gramaturas = r\"C:\\Users\\tbekho01.ATKEARNEY_AD\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\24. Handover\\250826_Otimização de Abastecedores_vEnviado\\Dados Raw\\Gramaturas\\Controle Gramaturas Oficial v_40.xlsx\"\n",
    "\n",
    "# -----------------------------\n",
    "# Função para converter nome do arquivo em DATA\n",
    "# Exemplo: \"0725\" -> 2025-07-01\n",
    "# -----------------------------\n",
    "def parse_nome_arquivo(nome_base):\n",
    "    mes = int(nome_base[:2])\n",
    "    ano = int(nome_base[2:])\n",
    "    ano = 2000 + ano if ano < 100 else ano  # converte 25 -> 2025\n",
    "    return pd.Timestamp(year=ano, month=mes, day=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Carregar SKUs válidos do Excel de Gramaturas\n",
    "# -----------------------------\n",
    "df_gram = pd.read_excel(arquivo_gramaturas, sheet_name=\"Planilha1\", skiprows=3, usecols=[0])\n",
    "lista_skus_validos = df_gram.iloc[:303, 0].dropna().astype(str).unique().tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# Processamento principal\n",
    "# -----------------------------\n",
    "arquivos_rateio = [f for f in os.listdir(pasta_rateio) if f.endswith(\".xlsx\")]\n",
    "\n",
    "lista_dfs = []\n",
    "\n",
    "for arquivo in arquivos_rateio:\n",
    "    nome_base, ext = os.path.splitext(arquivo)\n",
    "    caminho_rateio = os.path.join(pasta_rateio, arquivo)\n",
    "    caminho_telemetria = os.path.join(pasta_telemetria, arquivo)\n",
    "    \n",
    "    # --- Ler arquivo de Rateio ---\n",
    "    df_rateio = pd.read_excel(caminho_rateio)\n",
    "    df_rateio = df_rateio[df_rateio[\"FATURADO\"] == \"SIM\"]\n",
    "    df_rateio = df_rateio[[\"CODPARC\", \"NOMEPARC\", \"CODBEM\", \"CODPROD\", \"PRODUTO\", \"CONSUMO\"]].copy()\n",
    "    df_rateio[\"DATA\"] = parse_nome_arquivo(nome_base)\n",
    "    \n",
    "    # --- Ler arquivo de Telemetria correspondente ---\n",
    "    if os.path.exists(caminho_telemetria):\n",
    "        df_tel = pd.read_excel(caminho_telemetria)\n",
    "        df_tel.columns = df_tel.columns.str.lower()\n",
    "        \n",
    "        # Filtrar linhas onde id_patrimonio não está em CODBEM do rateio\n",
    "        df_tel = df_tel[~df_tel[\"id_patrimonio\"].isin(df_rateio[\"CODBEM\"])]\n",
    "        \n",
    "        # Selecionar colunas relevantes\n",
    "        df_tel = df_tel[[\"id_patrimonio\", \"sku\", \"produto\", \"quantidade_total\", \"nomeparc\"]]\n",
    "        \n",
    "        # Renomear colunas\n",
    "        df_tel = df_tel.rename(columns={\n",
    "            \"id_patrimonio\": \"CODBEM\",\n",
    "            \"sku\": \"CODPROD\",\n",
    "            \"produto\": \"PRODUTO\",\n",
    "            \"quantidade_total\": \"CONSUMO\",\n",
    "            \"nomeparc\": \"NOMEPARC\"\n",
    "        })\n",
    "        \n",
    "        # 🔑 Filtrar apenas SKUs válidos\n",
    "        df_tel = df_tel[df_tel[\"CODPROD\"].astype(str).isin(lista_skus_validos)]\n",
    "        \n",
    "        # Garantir coluna CODPARC\n",
    "        if \"CODPARC\" not in df_tel.columns:\n",
    "            df_tel[\"CODPARC\"] = None\n",
    "        \n",
    "        # Adicionar coluna DATA\n",
    "        df_tel[\"DATA\"] = df_rateio[\"DATA\"].iloc[0] if not df_rateio.empty else pd.NaT\n",
    "        \n",
    "        # Unir os dois DataFrames\n",
    "        df_unido = pd.concat([df_rateio, df_tel], ignore_index=True)\n",
    "    else:\n",
    "        df_unido = df_rateio\n",
    "    \n",
    "    lista_dfs.append(df_unido)\n",
    "\n",
    "# -----------------------------\n",
    "# Consolidação final\n",
    "# -----------------------------\n",
    "df_final = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "df_final[\"DATA\"] = df_final[\"DATA\"].dt.date\n",
    "\n",
    "# Salvar resultado consolidado\n",
    "saida = os.path.join(os.path.dirname(pasta_rateio), \"Consumo_Consolidado.xlsx\")\n",
    "df_final.to_excel(saida, index=False)\n",
    "\n",
    "print(f\"✅ Processamento concluído! Arquivo salvo em: {saida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidade de insumos por patrimônio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de padronização de string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_codes(code):\n",
    "    if str(code).replace('.','').isdigit():\n",
    "        return(str(int(code))).replace('_x000D_\\n', '').replace('\\n', '')\n",
    "    else:\n",
    "        return str(code).replace('_x000D_\\n', '').replace('\\n', '')\n",
    "    \n",
    "\n",
    "def std_patr(code):\n",
    "    if str(code).isdigit():\n",
    "        return str(int(code))\n",
    "    else:\n",
    "        return str(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_path = #r\"C:\\Users\\tbekho01\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\15. Fase de Acompanhamento\\Otimização de Abastecedores\\Rollout\\Rotas para ánalise_Rollout.xlsx\"\n",
    "lat_long_sp_path= #r\"C:\\Users\\tbekho01\\Kearney\\Gran Coffee - Otimização de despesas operacionais - Project Management\\5. Working Folder\\15. Fase de Acompanhamento\\Otimização de Abastecedores\\Rollout\\Endereços\\SP\\20250326_Lat_Long_SP.xlsx\"\n",
    "\n",
    "#Dados de consumo\n",
    "patrimonios_sp = pd.read_excel(dados_path, sheet_name='Consumo Consolidado SP')\n",
    "\n",
    "#Dados dos patrimonios\n",
    "rotas_sp = pd.read_excel(dados_path, sheet_name='Rotas SP', skiprows=1)\n",
    "\n",
    "#Latitude e Longitute dos parceiros\n",
    "lat_long_sp = pd.read_excel(lat_long_sp_path, sheet_name='Sheet1')\n",
    "\n",
    "#Capacidade das máquinas\n",
    "capacidade_df = pd.read_excel(dados_path, sheet_name='Capacidade', skiprows = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juntando os dados de consumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patrimonios_sp_clean=patrimonios_sp[[\"CODPARC\", \"NOMEPARC\", \"CODBEM\", \"CODPROD\", \"PRODUTO\",\"CONSUMO DOSES\", \"Data\"]]\n",
    "\n",
    "patrimonios_sp_clean=patrimonios_sp_clean.rename(columns={\"CONSUMO DOSES\": \"CONSUMO\", \"Data\": \"DATA\"})\n",
    "\n",
    "rotas_sp_clean=rotas_sp[[\"FILIAL\", \"PARCEIRO\", \"PATRIMÔNIO\", \"MODELO\", \"CLIENTE\",\"TIPO DE MAQUINA\", \"CAPACIDADE EM DOSES\"]]\n",
    "\n",
    "lat_long_sp_clean=lat_long_sp[[\"PARCEIRO\", \"PATRIMÔNIO\", \"Latitude\", \"Longitude\", \"CLIENTE AJUSTADO ÚNICO\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando padronização de strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patrimonios_sp_clean['NOMEPARC'] = patrimonios_sp_clean['NOMEPARC'].apply(std_codes)\n",
    "patrimonios_sp_clean['CODBEM'] = patrimonios_sp_clean['CODBEM'].apply(std_codes)\n",
    "patrimonios_sp_clean['CODBEM'] = patrimonios_sp_clean['CODBEM'].apply(std_patr)\n",
    "\n",
    "rotas_sp_clean['PARCEIRO'] = rotas_sp_clean['PARCEIRO'].apply(std_codes)\n",
    "rotas_sp_clean['PATRIMÔNIO'] = rotas_sp_clean['PATRIMÔNIO'].apply(std_codes)\n",
    "rotas_sp_clean['PATRIMÔNIO'] = rotas_sp_clean['PATRIMÔNIO'].apply(std_patr)\n",
    "\n",
    "lat_long_sp_clean['CLIENTE AJUSTADO ÚNICO'] = lat_long_sp_clean['CLIENTE AJUSTADO ÚNICO'].apply(std_codes)\n",
    "lat_long_sp_clean['PATRIMÔNIO'] = lat_long_sp_clean['PATRIMÔNIO'].apply(std_codes)\n",
    "lat_long_sp_clean['PATRIMÔNIO'] = lat_long_sp_clean['PATRIMÔNIO'].apply(std_patr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junção dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patrimonios_completo = pd.merge(lat_long_sp_clean, rotas_sp_clean, left_on='PATRIMÔNIO', right_on='PATRIMÔNIO', how='inner')\n",
    "\n",
    "patrimonios_completo=patrimonios_completo.drop([\"PARCEIRO_x\"], axis = 1)\n",
    "patrimonios_completo=patrimonios_completo.rename(columns={\"PARCEIRO_y\": \"PARCEIRO\"})\n",
    "\n",
    "capacidade_df_clean=capacidade_df.copy()\n",
    "capacidade_df_clean = capacidade_df_clean[[\"MODELO\", \"NOMENCLATURA SIMPLIFICADA\", \"CAFÉ GRÃO\", \"LEITE\", \"CHOCOLATE\", \"CHÁ\", \"CAFÉ SOLÚVEL\", \"AÇÚCAR\"]]\n",
    "\n",
    "patrimonios_completo_capacidade = pd.merge(patrimonios_completo, capacidade_df, on='MODELO', how='inner')\n",
    "patrimonios_completo_capacidade=patrimonios_completo_capacidade.rename(columns={\"CAPACIDADE EM DOSES\": \"COPOS\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas de insumos\n",
    "colunas_insumos = ['COPOS', 'CAFÉ GRÃO', 'LEITE', 'CHOCOLATE', 'CHÁ', 'CAFÉ SOLÚVEL', 'AÇÚCAR']\n",
    "\n",
    "# Usando melt para transformar as colunas de insumos em uma única coluna 'INSUMO'\n",
    "insumos = pd.melt(patrimonios_completo_capacidade, \n",
    "                  id_vars=['FILIAL', 'CLIENTE AJUSTADO ÚNICO', 'PATRIMÔNIO', 'MODELO'], \n",
    "                  value_vars=colunas_insumos,\n",
    "                  var_name='INSUMO', \n",
    "                  value_name='CAPACIDADE')\n",
    "\n",
    "# Renomeando a coluna 'CLIENTE AJUSTADO ÚNICO' para 'PARCEIRO'\n",
    "insumos['PARCEIRO'] = insumos['CLIENTE AJUSTADO ÚNICO']\n",
    "\n",
    "# Adicionando a coluna 'NIVEL DE REPOSIÇÃO' com o valor fixo de 0,3\n",
    "insumos['NIVEL DE REPOSIÇÃO'] = 0.3\n",
    "\n",
    "# Removendo a coluna 'CLIENTE AJUSTADO ÚNICO', pois já temos 'PARCEIRO'\n",
    "insumos = insumos.drop(columns=['CLIENTE AJUSTADO ÚNICO'])\n",
    "\n",
    "# Convertendo a coluna 'CAPACIDADE' para numérico, forçando erros para NaN\n",
    "insumos['CAPACIDADE'] = pd.to_numeric(insumos['CAPACIDADE'], errors='coerce')\n",
    "\n",
    "# Removendo as linhas onde 'CAPACIDADE' não é numérica (NaN)\n",
    "insumos = insumos.dropna(subset=['CAPACIDADE'])\n",
    "\n",
    "# Reorganizando as colunas na ordem desejada\n",
    "insumos = insumos[['FILIAL', 'PARCEIRO', 'PATRIMÔNIO', 'INSUMO', 'CAPACIDADE', 'NIVEL DE REPOSIÇÃO', 'MODELO']]\n",
    "\n",
    "# Exibindo o resultado\n",
    "insumos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando a aba \"insumos\" de Dados.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o novo arquivo de saída\n",
    "output_path = \n",
    "\n",
    "# Exportando o DataFrame 'insumos' para o novo arquivo Excel\n",
    "insumos.to_excel(output_path, index=False, sheet_name=\"insumos\")\n",
    "\n",
    "# Mensagem de confirmação\n",
    "print(f\"Arquivo exportado com sucesso para: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumo de insumos por patrimônio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado de gramaturas por doses\n",
    "gramaturas_sku = #pd.read_excel(dados_path, sheet_name='Gramaturas_concat_ROLLOUT', skiprows=2, usecols=\"B:R\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DATA column to datetime\n",
    "patrimonios_sp_clean['DATA'] = pd.to_datetime(patrimonios_sp_clean['DATA'])\n",
    "\n",
    "# Create INICIO (first day of the month)\n",
    "patrimonios_sp_clean['INICIO'] = patrimonios_sp_clean['DATA'].dt.to_period('M').dt.start_time\n",
    "\n",
    "# Create FIM (last day of the month)\n",
    "patrimonios_sp_clean['FIM'] = patrimonios_sp_clean['DATA'].dt.to_period('M').dt.end_time\n",
    "\n",
    "# Drop the original DATA column\n",
    "patrimonios_sp_clean = patrimonios_sp_clean.drop(columns=['DATA'])\n",
    "\n",
    "# Format INICIO and FIM to show only DD/MM/YYYY\n",
    "patrimonios_sp_clean['INICIO'] = patrimonios_sp_clean['INICIO'].dt.strftime('%d/%m/%Y')\n",
    "patrimonios_sp_clean['FIM'] = patrimonios_sp_clean['FIM'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Display result\n",
    "patrimonios_sp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(patrimonios_sp_clean, gramaturas_sku,\n",
    "                         left_on='CODPROD', right_on='SKU', how='inner')\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['DOSE FINAL (mL)']).drop([\"DOSE FINAL (mL)\", \"PALHETA\", \"Produto\", \"Obs\", \"Considerar como copo\", \"SKU\"], axis =1)\n",
    "\n",
    "insumos2 = ['CAFÉ GRÃO', 'LEITE', 'CHOCOLATE', 'CHÁ', 'CAFÉ SOLÚVEL', 'AÇÚCAR', \"CAFÉ COM LEITE CARAMELO\", \"CAPPUCCINO COM CANELA CAFÉ DO CENTRO\", \"CAPPUCCINO SEM CANELA CAFÉ DO CENTRO\", \"COPO\"]\n",
    "for insumos in insumos2:\n",
    "    merged_df[insumos] *= merged_df['CONSUMO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somar as colunas especificadas para esses CODBEMs\n",
    "soma_por_codbem = merged_df.groupby(['CODPARC','CODBEM', 'INICIO', 'FIM'])[['CAFÉ GRÃO', 'LEITE', 'CHOCOLATE', 'CHÁ', 'CAFÉ SOLÚVEL', 'AÇÚCAR', 'CAFÉ COM LEITE CARAMELO', 'CAPPUCCINO COM CANELA CAFÉ DO CENTRO', 'CAPPUCCINO SEM CANELA CAFÉ DO CENTRO', 'COPO']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pd.melt(soma_por_codbem, \n",
    "                    id_vars=['CODPARC', 'CODBEM', 'INICIO', 'FIM'],  # Colunas que ficam fixas\n",
    "                    value_vars=['CAFÉ GRÃO', 'LEITE', 'CHOCOLATE', 'CHÁ', 'CAFÉ SOLÚVEL', \n",
    "                                 'AÇÚCAR', 'CAFÉ COM LEITE CARAMELO', \n",
    "                                 'CAPPUCCINO COM CANELA CAFÉ DO CENTRO',\n",
    "                                 'CAPPUCCINO SEM CANELA CAFÉ DO CENTRO', 'COPO'],  # Colunas que vão virar INSUMO\n",
    "                    var_name='INSUMO',  # Nome da nova coluna para as variáveis de ingredientes\n",
    "                    value_name='CONSUMO')  # Nome da nova coluna para os valores\n",
    "\n",
    "# Renomeando as colunas conforme a estrutura desejada\n",
    "melted_df['FILIAL'] = \"SP\" # A coluna FILIAL será o CODPARC\n",
    "melted_df['PARCEIRO'] = melted_df['CODPARC']  # A coluna PARCEIRO será o CODBEM\n",
    "melted_df['PATRIMONIO'] = melted_df['CODBEM']  # PATRIMONIO também será o CODBEM (pode ser ajustado conforme a lógica)\n",
    "\n",
    "# Selecionando e reorganizando as colunas conforme a ordem desejada\n",
    "final_df = melted_df[['FILIAL', 'PARCEIRO', 'PATRIMONIO', 'INSUMO', 'CONSUMO', 'INICIO', 'FIM']]\n",
    "\n",
    "# Exibindo as primeiras linhas do DataFrame final\n",
    "final_df = final_df.sort_values(by=['FILIAL', 'PARCEIRO', 'PATRIMONIO',\"INICIO\", \"FIM\", 'INSUMO'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um ranking dos compartimentos de maior capacidade por patrimônio (para alocar os insumos especiais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 1. Filtrando os insumos desejados (CHÁ, CHOCOLATE, CAFÉ SOLÚVEL)\n",
    "insumos_filtrados = insumos[insumos['INSUMO'].isin(['CHÁ', 'CHOCOLATE', 'CAFÉ SOLÚVEL'])]\n",
    "\n",
    "# 2. Agrupando por PARCEIRO e PATRIMÔNIO, e ordenando por CAPACIDADE dentro de cada grupo\n",
    "# Vamos usar sort_values para ordenar por CAPACIDADE\n",
    "def ranking_insumos(group):\n",
    "    # Classificando por CAPACIDADE em ordem decrescente (do maior para o menor)\n",
    "    group_sorted = group.sort_values(by='CAPACIDADE', ascending=False)\n",
    "\n",
    "    # Criando o ranking de 1 a 3 para os insumos dentro do grupo\n",
    "    group_sorted['RANK'] = group_sorted['CAPACIDADE'].rank(ascending=False, method='first')\n",
    "\n",
    "    # Pegando as colunas de RANK 1, 2, 3\n",
    "    rank_1 = group_sorted[group_sorted['RANK'] == 1]['INSUMO'].values[0] if len(group_sorted[group_sorted['RANK'] == 1]) > 0 else None\n",
    "    rank_2 = group_sorted[group_sorted['RANK'] == 2]['INSUMO'].values[0] if len(group_sorted[group_sorted['RANK'] == 2]) > 0 else None\n",
    "    rank_3 = group_sorted[group_sorted['RANK'] == 3]['INSUMO'].values[0] if len(group_sorted[group_sorted['RANK'] == 3]) > 0 else None\n",
    "\n",
    "    # Retornando a linha com o PARCEIRO, PATRIMÔNIO e os 3 insumos no ranking\n",
    "    return pd.Series({\n",
    "        'PARCEIRO': group['PARCEIRO'].values[0],\n",
    "        'PATRIMÔNIO': group['PATRIMÔNIO'].values[0],\n",
    "        'CAPACIDADE_1': rank_1,\n",
    "        'CAPACIDADE_2': rank_2,\n",
    "        'CAPACIDADE_3': rank_3\n",
    "    })\n",
    "\n",
    "# 3. Aplicando a função para cada grupo de PARCEIRO e PATRIMÔNIO\n",
    "ranked_insumos = insumos_filtrados.groupby(['PARCEIRO', 'PATRIMÔNIO']).apply(ranking_insumos)\n",
    "\n",
    "# 4. Resetando o índice do DataFrame final para uma estrutura mais limpa\n",
    "ranked_insumos = ranked_insumos.reset_index(drop=True)\n",
    "\n",
    "# Mostrando o DataFrame final com os rankings\n",
    "ranked_insumos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alocando os insumos especiais nos compartimentos vazios dos patrimônios que possuem estes insumos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df_aux = final_df.copy()\n",
    "\n",
    "# Supondo que final_df_aux e ranked_insumos já estão definidos\n",
    "# final_df_aux é o dataframe principal, ranked_insumos é o dataframe com o ranking dos insumos\n",
    "\n",
    "# Lista de insumos a serem verificados\n",
    "target_insumos = ['CAFÉ COM LEITE CARAMELO', \n",
    "                  'CAPPUCCINO COM CANELA CAFÉ DO CENTRO',\n",
    "                  'CAPPUCCINO SEM CANELA CAFÉ DO CENTRO']\n",
    "\n",
    "# Passo 1: Filtrar as linhas onde o INSUMO é um dos alvos e o CONSUMO não é 0\n",
    "mask_target_insumos = final_df_aux['INSUMO'].isin(target_insumos)\n",
    "target_rows = final_df_aux[mask_target_insumos]\n",
    "\n",
    "# Passo 2: Processar cada linha\n",
    "rows_to_drop = []  # Para armazenar os índices das linhas a serem removidas\n",
    "\n",
    "for idx, row in target_rows.iterrows():\n",
    "    # Se CONSUMO for 0, ignorar e remover a linha\n",
    "    if row['CONSUMO'] == 0:\n",
    "        rows_to_drop.append(idx)\n",
    "        continue\n",
    "\n",
    "    # Passo 3: Verificar se CHÁ, CHOCOLATE ou CAFÉ SOLÚVEL têm CONSUMO igual a zero\n",
    "    zero_consumption_ingredients = []\n",
    "    \n",
    "    for ingredient in ['CHÁ', 'CHOCOLATE', 'CAFÉ SOLÚVEL']:\n",
    "        ingredient_row = final_df_aux[(final_df_aux['INSUMO'] == ingredient) & \n",
    "                                  #(final_df_aux['FILIAL'] == row['FILIAL']) & \n",
    "                                  (final_df_aux['PARCEIRO'] == row['PARCEIRO']) & \n",
    "                                  (final_df_aux['PATRIMONIO'] == row['PATRIMONIO']) & \n",
    "                                  (final_df_aux['INICIO'] == row['INICIO']) & \n",
    "                                  (final_df_aux['FIM'] == row['FIM'])]\n",
    "        \n",
    "        # Verificar se o CONSUMO é igual a zero e adicionar ao lista de insumos com consumo zero\n",
    "        if not ingredient_row.empty and ingredient_row['CONSUMO'].values[0] == 0:\n",
    "            zero_consumption_ingredients.append(ingredient)\n",
    "    \n",
    "    # Se mais de um insumo tiver CONSUMO igual a zero\n",
    "    if len(zero_consumption_ingredients) > 0:\n",
    "        # Passo 4: Verificar qual insumo tem o maior ranking de capacidade no ranked_insumos\n",
    "        print(zero_consumption_ingredients)\n",
    "        print(row['PATRIMONIO'])\n",
    "        \n",
    "        if row['PATRIMONIO'] in ranked_insumos[\"PATRIMÔNIO\"].unique():\n",
    "            print(\"s\")\n",
    "            insumo_cap_1 = ranked_insumos[ranked_insumos['PATRIMÔNIO'] == row['PATRIMONIO']]['CAPACIDADE_1'].values[0]\n",
    "            insumo_cap_2 = ranked_insumos[ranked_insumos['PATRIMÔNIO'] == row['PATRIMONIO']]['CAPACIDADE_2'].values[0]\n",
    "            insumo_cap_3 = ranked_insumos[ranked_insumos['PATRIMÔNIO'] == row['PATRIMONIO']]['CAPACIDADE_3'].values[0]\n",
    "            \n",
    "            if insumo_cap_1 in zero_consumption_ingredients:\n",
    "                target_ingredient = insumo_cap_1 \n",
    "            elif insumo_cap_2 in zero_consumption_ingredients:\n",
    "                target_ingredient = insumo_cap_2\n",
    "            elif insumo_cap_3 in zero_consumption_ingredients:\n",
    "                target_ingredient = insumo_cap_3 \n",
    "\n",
    "            ingredient_row = final_df_aux[(final_df_aux['INSUMO'] == target_ingredient) & \n",
    "                                #(final_df_aux['FILIAL'] == row['FILIAL']) & \n",
    "                                (final_df_aux['PARCEIRO'] == row['PARCEIRO']) & \n",
    "                                (final_df_aux['PATRIMONIO'] == row['PATRIMONIO']) & \n",
    "                                (final_df_aux['INICIO'] == row['INICIO']) & \n",
    "                                (final_df_aux['FIM'] == row['FIM'])]\n",
    "            \n",
    "            # Transferir o valor de CONSUMO para o insumo com maior ranking\n",
    "            final_df_aux.loc[ingredient_row.index, 'CONSUMO'] = row['CONSUMO']\n",
    "            final_df_aux.loc[ingredient_row.index, 'TRANSFER'] = row[\"INSUMO\"]\n",
    "            \n",
    "            # Após a transferência, marcar a linha original para exclusão\n",
    "            rows_to_drop.append(idx)\n",
    "        else:\n",
    "            print(\"n\")\n",
    "            rows_to_drop.append(idx)\n",
    "\n",
    "# Passo 5: Deletar as linhas marcadas para exclusão\n",
    "final_df_aux = final_df_aux.drop(rows_to_drop)\n",
    "\n",
    "# Resetar o índice\n",
    "final_df_aux = final_df_aux.reset_index(drop=True)\n",
    "\n",
    "# Mostrar o DataFrame final\n",
    "final_df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retirando as linhas em que o consumo é 0\n",
    "final_df_aux_export= final_df_aux[final_df_aux[\"CONSUMO\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando Dados de Consumo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caminho de saída\n",
    "output_path = #, 'Dados de Consumo.csv')\n",
    "\n",
    "# Exportar para CSV com encoding UTF-8\n",
    "final_df_aux_export.to_csv(output_path, index=False, encoding='cp1252', sep=';')\n",
    "\n",
    "# Mensagem de confirmação\n",
    "print(f\"Arquivo consolidado exportado com sucesso para: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
